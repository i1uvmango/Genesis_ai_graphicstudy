
## 실제 코드 구현

### CurriculumScheduler 클래스

```python
class CurriculumScheduler:
    """Curriculum Learning scheduler with warmup for progress/rate penalties."""
    
    def __init__(self, total_steps: int = 50000, warmup_iters: int = 30):
        self.total_steps = total_steps
        self.warmup_iters = warmup_iters
    
    def get_w_dir(self, iteration: int) -> float:
        """Direction reward weight: 0.1 → 0.3"""
        progress = min(iteration / 100, 1.0)  # ~100 iterations to full
        return 0.1 + 0.2 * progress
    
    def process_R_dir(self, R_dir: torch.Tensor, iteration: int) -> torch.Tensor:
        """Clamp R_dir early in training."""
        if iteration < 50:
            return torch.clamp(R_dir, 0, 1)
        return R_dir
    
    def get_progress_weight(self, iteration: int) -> float:
        """Progress reward weight: 0 for first warmup_iters, then ramp to 1.0"""
        if iteration < self.warmup_iters:
            return 0.0
        # Ramp from 0 to 1 over next warmup_iters
        ramp = min((iteration - self.warmup_iters) / self.warmup_iters, 1.0)
        return ramp
    
    def get_rate_weight(self, iteration: int) -> float:
        """Rate penalty weight: 0 for first warmup_iters, then ramp to 1.0"""
        if iteration < self.warmup_iters:
            return 0.0
        ramp = min((iteration - self.warmup_iters) / self.warmup_iters, 1.0)
        return ramp
    
    def get_brake_multiplier(self, iteration: int) -> float:
        """Brake torque curriculum: 0.5 at start, ramps to 1.0
        
        Design: Start with limited brake to encourage forward motion.
        As training progresses, allow full braking for speed control.
        """
        # Ramp from 0.5 to 1.0 over first 60 iterations
        ramp = min(iteration / 60.0, 1.0)
        return 0.5 + 0.5 * ramp
```

### Tracking Reward 함수

```python
def compute_tracking_reward_batch(target_rel: torch.Tensor, v_body: torch.Tensor,
                                   step: int, scheduler: CurriculumScheduler) -> torch.Tensor:
    """Batch tracking reward."""
    w_dist = 1.0
    w_dir = scheduler.get_w_dir(step)
    
    # Distance reward
    d = torch.norm(target_rel, dim=1)
    R_dist = torch.exp(-d)
    
    # Direction reward (XY plane only for 3D stability)
    v_xy = v_body[:, :2]
    t_xy = target_rel[:, :2]
    
    speed_xy = torch.norm(v_xy, dim=1)
    t_norm_xy = torch.norm(t_xy, dim=1)
    
    v_hat = v_xy / (speed_xy.unsqueeze(-1) + 1e-6)
    t_hat = t_xy / (t_norm_xy.unsqueeze(-1) + 1e-6)
    
    R_dir = torch.sum(v_hat * t_hat, dim=1)
    R_dir = torch.where(speed_xy > 0.1, R_dir, torch.zeros_like(R_dir))
    R_dir = scheduler.process_R_dir(R_dir, step)
    
    return w_dist * R_dist + w_dir * R_dir
```

### Progress Reward 함수

```python
def compute_progress_reward_batch(d_prev: torch.Tensor, d_curr: torch.Tensor,
                                   delta_max: float = 0.5) -> torch.Tensor:
    """Batch progress reward."""
    progress = d_prev - d_curr
    return torch.clamp(progress, -delta_max, delta_max)
```

### Total Reward 함수 (핵심)

```python
def compute_total_reward_batch(target_rel: torch.Tensor, v_body: torch.Tensor,
                                steer: torch.Tensor, steer_prev: torch.Tensor,
                                iteration: int, scheduler: CurriculumScheduler,
                                config: Config,
                                tangent_rel: torch.Tensor,
                                prev_lat_error: torch.Tensor,
                                arc_length: torch.Tensor,
                                progress_idx: torch.Tensor,
                                prev_progress_idx: torch.Tensor,
                                nearest_rel: torch.Tensor) -> torch.Tensor:
    """
    Batch total reward computation with Explicit Steering Strategy.
    
    1. R_align (1st Order): "Steer towards the target" (Lookahead)
       - target_rel.y * steer
       
    2. R_recover (2nd Order): "Did you reduce the error?" (Current)
       - |e_prev| - |e_curr| (using nearest_rel)
       
    3. R_proj & R_arc (Dual Progress): "Move forward"
    """
    # 1. Alignment Reward (1st Order Steering Command)
    # Target is at target_rel (Body Frame).
    # If target is Left (y>0), Steer Left (>0) -> Reward > 0
    # Limit reward to avoid overpowering
    R_align = torch.clamp(target_rel[:, 1] * steer, -1.0, 1.0)
    
    # 2. Recovery Reward (2nd Order Steering Command)
    # validation: did current error decrease?
    # Use NEAREST_REL for current error (true cross track error)
    curr_lat_error_abs = torch.abs(nearest_rel[:, 1])
    error_diff = prev_lat_error - curr_lat_error_abs
    R_recover = torch.clamp(error_diff, -0.2, 0.2)
    
    # 3. Progress Rewards
    # 3.1 Projection (Speed/Dir) relative to Path
    proj_speed = v_body[:, 0] * tangent_rel[:, 0] + v_body[:, 1] * tangent_rel[:, 1]
    R_proj = torch.clamp(proj_speed / config.target_speed, -0.2, 0.5)
    
    # 3.2 Arc-length (Absolute Mileage) - PREVENTS STOPPING
    s_curr = arc_length[progress_idx]
    s_prev = arc_length[prev_progress_idx]
    R_arc = torch.clamp(s_curr - s_prev, min=0.0, max=0.5)
    
    # 4. Forward reward (Auxiliary)
    v_long = v_body[:, 0]
    R_forward = torch.tanh(v_long)
    
    # Penalties
    P_steer = steer ** 2
    P_rate = torch.abs(steer - steer_prev)
    P_speed = torch.clamp(v_long - config.target_speed, min=0.0)
    P_stuck = torch.clamp(0.1 - v_long, min=0.0)
    
    # NEW: P_lat (squared lateral error penalty)
    lat_err = torch.abs(nearest_rel[:, 1])
    lat_err_clamped = torch.clamp(lat_err, 0.0, 2.0)
    P_lat = lat_err_clamped ** 2  # max 4.0
    
    # Curriculum weights
    w_progress = config.w_progress * scheduler.get_progress_weight(iteration)
    w_arc = config.w_arc * scheduler.get_progress_weight(iteration) 
    w_rate = config.w_rate * scheduler.get_rate_weight(iteration)
    
    # R_recover is critical
    w_recover = 2.0
    
    # w_track curriculum: 0.5 -> 1.0
    w_track = config.w_track * scheduler.get_track_weight(iteration)
    
    total_reward = (
        R_align
        + w_recover * R_recover
        + w_progress * R_proj
        + w_arc * R_arc
        + config.w_forward * R_forward
        - w_track * P_lat  # NEW: squared lateral error
        - config.w_steer * P_steer
        - w_rate * P_rate
        - config.w_speed * P_speed
        - config.w_stuck * P_stuck
    )
    
    return total_reward
```

### Done Conditions (에피소드 종료 조건)

```python
def get_irrecoverable_thresholds(step: int, curriculum_steps: int = 200000) -> dict:
    """Get curriculum-adjusted thresholds."""
    progress = min(step / curriculum_steps, 1.0)
    y_max = 5.0 - 2.0 * progress
    omega_max = 3.0 - 1.0 * progress
    return {'y_max': y_max, 'omega_max': omega_max}


def check_done_batch(pos_car: torch.Tensor, path_points: torch.Tensor,
                      target_idx: torch.Tensor, target_rel: torch.Tensor,
                      v_body: torch.Tensor, omega_body: torch.Tensor,
                      episode_lengths: torch.Tensor, global_step: int,
                      config: Config) -> torch.Tensor:
    """
    Batch done condition check.
    Returns: done_mask (N,) boolean tensor
    """
    N = pos_car.shape[0]
    M = path_points.shape[0]
    device = pos_car.device
    
    done = torch.zeros(N, dtype=torch.bool, device=device)
    
    # 1. Goal reached
    done = done | (target_idx >= M - 1)
    
    # 2. Off-track
    distances = torch.norm(path_points.unsqueeze(0) - pos_car.unsqueeze(1), dim=2)
    min_dist = torch.min(distances, dim=1).values
    done = done | (min_dist > config.off_track_distance)
    
    # 3. Reverse Motion = Invalid State (not penalized, just terminated)
    # Design: Reverse is a forbidden state, not a target behavior
    done = done | (v_body[:, 0] < config.reverse_velocity_threshold)
    
    # 4. Irrecoverable (curriculum)
    thresholds = get_irrecoverable_thresholds(global_step, config.irrecoverable_curriculum_steps)
    lateral_error = torch.abs(target_rel[:, 1])
    yaw_rate = torch.abs(omega_body[:, 2])
    irrecoverable = (lateral_error > thresholds['y_max']) & (yaw_rate > thresholds['omega_max'])
    done = done | irrecoverable
    
    # 5. Timeout
    done = done | (episode_lengths >= config.max_episode_length)
    
    return done
```
# PPO 보상 함수 설계 

## 개요

PPO Direct Control 시스템에서 사용되는 보상 함수 설계를 정리합니다.

### 설계 철학
- **브레이크는 보상 대상이 아님**: 브레이크는 과속 페널티 및 회복 불가능 상태를 피하기 위한 수단으로 간접적으로 학습됨
- **후진은 금지 상태**: 후진 동작은 목표 행동이 아닌 무효 상태로 처리 (에피소드 종료)

---

## 보상 가중치 (Config)

| 변수 | 값 | 설명 |
|------|------|------|
| `w_track` | **1.5** | 횡방향 오차 (P_lat², with curriculum) |
| `w_progress` | 1.0 | 진행 보상 (v_long > 0일 때만, Projection) |
| `w_arc` | 1.0 | 절대 arc-length 진행 |
| `w_forward` | 0.3 | 전진 보상 (v_long > 0일 때만) |
| `w_steer` | 0.2 | 스티어링 크기 페널티 |
| `w_rate` | 0.1 | 스티어링 변화율 페널티 |
| `w_speed` | 0.3 | 목표 속도 초과 시 페널티 |
| `w_stuck` | 1.0 | 정체 페널티 (v_long < 0.1) |
| `lookahead` | **3.0m** | Arc-length lookahead distance |

---

## 커리큘럼 학습 (CurriculumScheduler)

학습 초기에는 일부 보상/페널티를 비활성화하고, iteration이 진행됨에 따라 점진적으로 활성화합니다.

### 주요 메서드

| 메서드 | 동작 |
|--------|------|
| `get_w_dir(iteration)` | 방향 보상 가중치: `0.1 → 0.3` (100 iteration에 걸쳐) |
| `get_progress_weight(iteration)` | warmup 기간(30 iter) 동안 0, 이후 1.0까지 ramp-up |
| `get_rate_weight(iteration)` | warmup 기간(30 iter) 동안 0, 이후 1.0까지 ramp-up |
| `get_track_weight(iteration)` | **w_track 커리큘럼: `0.5 → 1.0`** |
| `get_brake_multiplier(iteration)` | 브레이크 토크: `0.5 → 1.0` (60 iter에 걸쳐) |

---

## 기준점 convention
![](../res/0112/point.png)
![](../res/0112/err4.png)
* lookahead point : steering 을 lookahead point를 기준으로 미리 조절하여 부드러운 steering 을 만듦
* nearest point : 차량과 가장 가까운 경로 지점


## 총 보상 함수 구조

### `compute_total_reward_batch()` 

```
Total Reward = R_align 
             + w_recover × R_recover 
             + w_progress × R_proj 
             + w_arc × R_arc
             + w_forward × R_forward
             - w_track × P_lat²       ← NEW (squared lateral error)
             - w_steer × P_steer
             - w_rate × P_rate  
             - w_speed × P_speed
             - w_stuck × P_stuck
```

---

## 개별 보상 항목

### 1. Alignment Reward (R_align) - 조향 의도 보상

![](../res/0112/err2.png)

> **"목표 방향으로 조향하라"**
* target_point(빨간 공)가 있는 방향으로 steering이 일어나는가?


```python
R_align = clamp(target_rel.y × steer, -1.0, 1.0)
```

- `target_rel.y`: Body Frame 기준 목표점의 y 좌표 (좌/우 편차)
- 목표가 왼쪽(y > 0)이면 왼쪽 조향(steer > 0) → 양의 보상
- 목표가 오른쪽(y < 0)이면 오른쪽 조향(steer < 0) → 양의 보상
##### 목표 방향에 맞는 조향을 하면 보상을 줌

---
### 2. Recovery Reward (R_recover) - 횡오차 감소 보상

![](../res/0112/err1.png)

> **"횡방향 오차(e_lat)를 줄였는가?"**
* 좌우 shift err 줄였는가? 
* nearest_point 기준
* 로컬(단기) 가이드 신호



```python
curr_lat_error = |nearest_rel.y|
R_recover = clamp(prev_lat_error - curr_lat_error, -0.2, 0.2)
```

- **가중치**: `w_recover = 2.0` (고정, 가장 중요)
- 현재 프레임에서 횡방향 오차가 감소하면 양의 보상
- `nearest_rel`: 가장 가까운 경로 지점 기준 (true cross-track error)


---
### 3. Projection Reward (R_proj) - 경로 방향 속도 성분 보상

![](../res/0112/err3.png)

> **"경로 접선 방향으로 이동하라"**
* nearest_point의 접선 방향으로 차량이 이동하는가?
* 출력/속도 에 대한 보상

```python
proj_speed = v_body.x × tangent.x + v_body.y × tangent.y
R_proj = clamp(proj_speed / target_speed, -0.2, 0.5)
```

- 차량 속도를 경로 접선(tangent) 방향으로 투영
- Curriculum: 초기에는 비활성화, 점진적으로 활성화
    * 학습 안정성을 위해 점진적으로 활성화 하여 페널티로 인한 느린 학습을 방지
---
### 4. Arc-Length Reward (R_arc) - 누적 경로 거리에 따른 보상

> **"정지하지 말고 전진하라"**

```python
R_arc = clamp(s_curr - s_prev, 0, 0.5)
```

- `arc_length[progress_idx]`: 누적 경로 거리
- **정지 방지 핵심 보상**: 실제로 경로상에서 얼마나 진행했는지 측정

#### 이게 왜 필요한가?
`위 보상함수들은 다음 조건만 맞추면 된다`

* R_align: lookahead point 방향으로 조향만 맞추면 OK
* R_recover(e_lat): 붙지 않으면 손해
* R_proj: 접선 방향 속도 성분만 있으면 OK

다음과 같은 로컬 최적해에 도달 할 수 있음(움직이지 않음)
> 보상 정의의 빈틈을 메꿔주는 역할
```
아예 거의 멈춰서
방향만 살짝 맞추고
e_lat 줄이는 게 최고다”
```

* R_arc 를 통해 `앞으로 가지 않으면 보상이 0이다`를 명시해서 차량이 계속 움직이게끔 함(정지상태 거부)


### 5. Forward Reward (R_forward) - 전진 보조

#### 차량이 앞으로 움직이면 R_forward를 통해 보상을 주도록 함
- 전진 속도에 비례하는 보조 보상
- 너무 느린 속도를 방지하기 위함
- 그렇다고 빠른 속도 만으로 보상을 얻으면 안됨 &rarr; tanh 함수 사용

![](../res/0112/err5.png)

```python
R_forward = tanh(v_long)
```
* `tanh` 를 씌우는 이유는 보상을 위해 무작정 속도(v_long)을 키우는 것을 방지하기 위해 [-1,1]로 스케일링
- 가중치: `w_forward = 0.3`



---

## 페널티 항목

### 1. Steering Penalty (P_steer)

> **"최소한의 조향을 사용하라"**

```python
P_steer = steer²
```
* P_steer [-1,1] 이므로 steering이 클수록 페널티 큼


##### 무엇을 규제하나?

* 조향 입력의 크기 자체

* 핸들을 얼마나 많이 꺾었는지

##### 어떤 행동을 억제하나?

* 불필요하게 큰 조향 유지
* 경로에 붙었는데도 큰 조향을 계속 주는 습관

##### 어떤 문제를 해결하나?

* steady-state에서의 과도한 조향

* 경로 추종 후에도 조향이 남아 있는 현상

> 경로 이탈이 큰 상황에서 필요한 큰 조향 입력까지 억제하여
경로 복귀를 방해하지 않는가?”

* 조향을 하지 않아 발생하는 손실(P_lat)이 조향을 수행하여 발생하는 손실(P_steer)보다 항상 크도록 설계되어 있음
* (p_steer 가중치 0.05  vs p_lat 가중치 최대 4.0 )




### 2. Rate Penalty (P_rate)

> **"조향 변화를 부드럽게 하라"**

```python
P_rate = |steer - steer_prev|
```
- 급격한 조향 변화 억제 (부드러운 제어 유도)
- Curriculum: 초기에는 비활성화

##### 무엇을 규제하나?
* 조향 입력의 변화량 (시간적 변화)

##### 어떤 행동을 억제하나?

* 좌 ↔ 우로 빠르게 흔드는 조향
* 고주파 진동(wobbling)

##### 어떤 문제를 해결하나?
* 제어 입력의 불연속성
* 실제 차량에서 불안정·비현실적 움직임


### 3. Speed Penalty (P_speed)

```python
P_speed = clamp(v_long - target_speed, 0, ∞)
```
- 목표 속도 초과 시에만 페널티

### 4. Stuck Penalty (P_stuck)

```python
P_stuck = clamp(0.1 - v_long, 0, ∞)
```
- 속도가 0.1 m/s 미만이면 페널티 (정체 방지)

### 5. Lateral Error Penalty (P_lat)

> **"경로에서 벗어나지 마라"**
* 보상 목록 1번 R_recover 과 같은 내용

```python
lat_err_clamped = clamp(|nearest_rel.y|, 0.0, 2.0)
P_lat = lat_err_clamped ** 2  # 최대 4.0
```

- **제곱 형태**: 작은 오차 관대, 큰 오차 강력 페널티
- **커리큘럼**: w_track × (0.5 → 1.0) 점진적 증가
- `nearest_rel.y`: nearest_point 기준 횡방향 에러 err_lat
* 글로벌(장기) 가이드 신호: R_recover만으로는 제거되지 않는 장기적 경로 이탈


##### 무엇을 규제하나?

* 차량의 경로 대비 횡방향 위치 오차 (cross-track error)
* 조향 입력이 아니라, 조향의 결과로 나타난 상태(state)

##### 어떤 행동을 억제하나?

* 경로에서 벗어난 채로 조향을 하지 않거나
* 조향 대신 속도를 줄이거나 정체하여 페널티를 회피하는 행동

##### 어떤 문제를 해결하나?

* “멈추거나 느리게 가서 손해를 회피하는” 퇴행적 정책(degenerate policy)
* 조향을 최소화하면서 경로 복귀를 회피하는 under-steering 문제
* 조향 페널티(P_steer, P_rate)만으로는 강제할 수 없는 경로 추종 자체에 대한 필수 동기 부여

### R_recover vs P_lat
> R_recover: “방금 한 조향, 잘했어?”  (단기)  
> P_lat: “지금 위치, 규칙 어겼어?”  (장기)
---

## 에피소드 종료 조건 (Done Conditions)

| 조건 | 기준 | 설명 |
|------|------|------|
| **Goal Reached** | `target_idx >= M - 1` | 목표 도달 |
| **Off-Track** | 경로 최단 거리 > `2.0m` | 2m 이상 경로 벗어남 |
| **Reverse Motion** | `v_long < -0.3 m/s` | 역주행 |
| **Irrecoverable** | 횡오차 > threshold 기준보다 큼 or 바퀴 yaw_rate > threshold | 복구 불가능 |
| **Timeout** | `episode_length >= 500` | 시간 초과 |


### Irrecoverable Thresholds (Curriculum)

```python
y_max = 5.0 - 2.0 × progress    # 5.0m → 3.0m
omega_max = 3.0 - 1.0 × progress  # 3.0 rad/s → 2.0 rad/s
```
- 점진적 적용: 학습 초기에는 관대하게, 후반에는 엄격하게 적용

---

## mlp input / 관찰 공간 (Observation) - 18차원

| 변수 | 차원 | 설명 |
|------|------|------|
| `target_rel` | 3 | Body Frame 기준 목표 waypoint 상대 위치 |
| `v_body` | 3 | Body Frame 기준 속도 (vx, vy, vz) |
| `omega_body` | 3 | Body Frame 기준 각속도 |
| `g_body` | 3 | Body Frame 기준 중력 벡터 |
| `tangent_rel` | 3 | Body Frame 기준 경로 접선 방향 |
| `slip_proxy` | 1 | 슬립 근사값 (throttle_prev - v_norm) |
| `prev_action` | 2 | 이전 step action (throttle, steer) |

---
